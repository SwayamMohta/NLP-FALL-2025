{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# NLP Class Activity 2: Documentation for Byte-Pair Encoding\n",
        "\n",
        "### **Objective**\n",
        "This notebook provides a step-by-step implementation of the **Byte-Pair Encoding (BPE)** algorithm from scratch."
      ],
      "metadata": {
        "id": "AyJ9MrsY9M0t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **How BPE Works: The Core Algorithm**\n",
        "The BPE algorithm follows a simple, iterative process:\n",
        "1.  **Initialize Vocabulary**: Start with a base vocabulary containing every individual character present in the text.\n",
        "2.  **Find Most Frequent Pair**: Scan the entire text corpus and identify the pair of adjacent tokens that appears most frequently.\n",
        "3.  **Merge the Pair**: Create a new token by merging this pair. Add this new token to your vocabulary.\n",
        "4.  **Repeat**: Continue finding and merging the next most frequent pair for a predetermined number of iterations. With each merge, the vocabulary grows to include more complex subwords (e.g., from `t` and `h` to `th`, then from `th` and `e` to `the`)."
      ],
      "metadata": {
        "id": "kVQx45Cg9VJE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter, defaultdict\n",
        "import re"
      ],
      "metadata": {
        "id": "CR_RxyCs8ggO"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Code Implementation Breakdown**\n",
        "\n",
        "### **Step 1: Load the Text Corpus**\n",
        "The code begins by downloading the \"Tiny Shakespeare\" dataset, which is a single plain text file containing the complete works of Shakespeare. This text serves as our training data, from which the BPE algorithm will learn its vocabulary."
      ],
      "metadata": {
        "id": "g1GZUOqi9kU3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt -O tinyshakespeare.txt\n",
        "\n",
        "with open(\"tinyshakespeare.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    corpus = f.read()\n",
        "\n",
        "print(\"Corpus loaded!\")\n",
        "print(\"Corpus length (characters):\", len(corpus))\n",
        "print(\"Sample text:\\n\", corpus[:200])"
      ],
      "metadata": {
        "id": "3ZFttbLo9Y84"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 2: Create the Initial Vocabulary**\n",
        "Before the main BPE process can start, we need to \"pre-tokenize\" the corpus. The `get_vocab` function does this:\n",
        "* It takes each word from the text (e.g., \"First\").\n",
        "* It splits the word into individual characters (`'F', 'i', 'r', 's', 't'`).\n",
        "* It adds a special **end-of-word token** `</w>` to the end. This is crucial as it allows the model to understand where a word ends. The result for \"First\" would be `F i r s t </w>`.\n",
        "* The initial vocabulary is a dictionary counting the frequency of each of these character-based words."
      ],
      "metadata": {
        "id": "p_Q3Tvcn9mT5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eT1ZybSOsx9W"
      },
      "outputs": [],
      "source": [
        "def get_vocab(text):\n",
        "    vocab = Counter()\n",
        "    for word in text.split():\n",
        "        vocab[\" \".join(list(word)) + \" </w>\"] += 1\n",
        "    return vocab\n",
        "\n",
        "vocab = get_vocab(corpus[:10000])\n",
        "print(\"\\n Sample vocabulary entries (word -> frequency):\")\n",
        "for i, (w, f) in enumerate(vocab.items()):\n",
        "    if i > 5: break\n",
        "    print(w, \":\", f)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 3: The BPE Loop (Finding and Merging Pairs)**\n",
        "This is the core of the algorithm, implemented in the `byte_pair_encoding` function. It runs a loop for a set number of merges (in this case, 10 for the demo). In each iteration:\n",
        "\n",
        "1.  **Get Pair Statistics (`get_stats`)**:\n",
        "2.  **Find the Best Pair**:\n",
        "3.  **Merge the Vocabulary (`merge_vocab`)**:"
      ],
      "metadata": {
        "id": "RTnPursq9t2i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_stats(vocab):\n",
        "    pairs = defaultdict(int)\n",
        "    for word, freq in vocab.items():\n",
        "        symbols = word.split()\n",
        "        for i in range(len(symbols)-1):\n",
        "            pairs[(symbols[i], symbols[i+1])] += freq\n",
        "    return pairs\n",
        "\n",
        "\n",
        "def merge_vocab(pair, vocab):\n",
        "    pattern = re.escape(' '.join(pair))\n",
        "    pattern = re.compile(r'(?<!\\S)' + pattern + r'(?!\\S)')\n",
        "    new_vocab = {}\n",
        "    for word in vocab:\n",
        "        new_word = pattern.sub(''.join(pair), word)\n",
        "        new_vocab[new_word] = vocab[word]\n",
        "    return new_vocab"
      ],
      "metadata": {
        "id": "DcHygfcL8fWy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 4: The Final Result**\n",
        "After the loop completes, the script prints the list of `merges` that were learned. This list **is the learned BPE vocabulary**. Each tuple, like `('t', 'h')`, represents a merge rule. These learned subwords can now be used to tokenize new text. For example, a new, unseen word like \"therefore\" could be broken down into the known subwords `th`, `er`, `ef`, `or`, `e`.\n"
      ],
      "metadata": {
        "id": "8no17lXh93ZL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def byte_pair_encoding(vocab, num_merges=10):\n",
        "    merges = []\n",
        "    for i in range(num_merges):\n",
        "        pairs = get_stats(vocab)\n",
        "        if not pairs:\n",
        "            break\n",
        "        best = max(pairs, key=pairs.get)\n",
        "        vocab = merge_vocab(best, vocab)\n",
        "        merges.append(best)\n",
        "        print(f\"Step {i+1}: Merge {best}\")\n",
        "    return vocab, merges\n",
        "\n",
        "final_vocab, merges = byte_pair_encoding(vocab, num_merges=15)\n",
        "\n",
        "print(merges)"
      ],
      "metadata": {
        "id": "xKHZYZxE99f5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}